{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMiGeieMXuEmYskyCo5eUo3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"9OH_KYR5f9XA","colab_type":"code","outputId":"f3b18caa-f793-4d85-833c-a40f1a4edada","executionInfo":{"status":"ok","timestamp":1588557539644,"user_tz":300,"elapsed":28834,"user":{"displayName":"Jonathan Ou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2yyYoMRykV8sYjocGFoZDy0c7Y1DdSb0H74orJQ=s64","userId":"01702736591762830230"}},"colab":{"base_uri":"https://localhost:8080/","height":561}},"source":["# Libraries Needed\n","!pip install wandb -q\n","!pip install gym\n","!apt-get install python-opengl -y\n","!apt install xvfb -y\n","!pip install pyvirtualdisplay\n","!pip install piglet\n","!apt-get install x11-utils"],"execution_count":46,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","python-opengl is already the newest version (3.1.0+dfsg-1).\n","0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","xvfb is already the newest version (2:1.19.6-1ubuntu4.4).\n","0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n","Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (0.2.5)\n","Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n","Requirement already satisfied: piglet in /usr/local/lib/python3.6/dist-packages (1.0.0)\n","Requirement already satisfied: piglet-templates in /usr/local/lib/python3.6/dist-packages (from piglet) (1.0.0)\n","Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n","Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.6.3)\n","Requirement already satisfied: Parsley in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.3)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (19.3.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.34.2)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.12.0)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","x11-utils is already the newest version (7.7+3build1).\n","0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l9ZGDzLQyb8Z","colab_type":"text"},"source":["###Run Settings"]},{"cell_type":"code","metadata":{"id":"gap9qGx2ydeU","colab_type":"code","colab":{}},"source":["# wandb metric tracking\n","log_metrics = True\n","\n","# Cuda\n","use_cuda = True \n","\n","# Google Colab\n","use_colab = True"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q55hg8XkyXW-","colab_type":"text"},"source":["### Import Libraries"]},{"cell_type":"code","metadata":{"id":"u-Ava8RSgKZe","colab_type":"code","colab":{}},"source":["# Import Libraries\n","import wandb\n","\n","import configparser\n","import gym\n","from gym import wrappers\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.autograd as autograd\n","import torch.nn.functional as F\n","\n","from collections import deque\n","\n","import random"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i4-wL7Y2RMsC","colab_type":"text"},"source":["Google Colab Setup"]},{"cell_type":"code","metadata":{"id":"gX15wOLaROzA","colab_type":"code","outputId":"99b7b73a-a579-4a10-a115-af64080d3cc6","executionInfo":{"status":"ok","timestamp":1588557539646,"user_tz":300,"elapsed":2682,"user":{"displayName":"Jonathan Ou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2yyYoMRykV8sYjocGFoZDy0c7Y1DdSb0H74orJQ=s64","userId":"01702736591762830230"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":49,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sIKdx8O2Rmpj","colab_type":"text"},"source":["Variable Setup"]},{"cell_type":"code","metadata":{"id":"l_zaddL6RpjI","colab_type":"code","colab":{}},"source":["project_dir = \"/content/gdrive/My Drive/Colab Notebooks/Paper Implementation/Playing-Atari-with-Deep-Reinforcement-Learning/\"\n","config_ini = project_dir + '/config.ini'\n","video_dir = project_dir + '/videos'\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FYTbEdvsymCr","colab_type":"text"},"source":["### Metric Tracking Setup"]},{"cell_type":"code","metadata":{"id":"2WHmsfncrFk8","colab_type":"code","outputId":"547219b9-5b4b-43f5-dfc7-96634b8d8638","executionInfo":{"status":"ok","timestamp":1588557545916,"user_tz":300,"elapsed":4869,"user":{"displayName":"Jonathan Ou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2yyYoMRykV8sYjocGFoZDy0c7Y1DdSb0H74orJQ=s64","userId":"01702736591762830230"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["  # Wandb Login\n","if log_metrics:\n","  config = configparser.ConfigParser()\n","  config.sections()\n","  config.read(config_ini)\n","\n","  login = config['WANDB']['login']\n","  entity = config['WANDB']['entity']\n","  project = config['WANDB']['project']\n","\n","  wandb.login(login)\n","  wandb.init(entity=entity, project=project, anonymous='never')"],"execution_count":51,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n","                Project page: <a href=\"https://app.wandb.ai/oumilk/Playing-Atari-with-Deep-Reinforcement-Learning\" target=\"_blank\">https://app.wandb.ai/oumilk/Playing-Atari-with-Deep-Reinforcement-Learning</a><br/>\n","                Run page: <a href=\"https://app.wandb.ai/oumilk/Playing-Atari-with-Deep-Reinforcement-Learning/runs/30dn21l0\" target=\"_blank\">https://app.wandb.ai/oumilk/Playing-Atari-with-Deep-Reinforcement-Learning/runs/30dn21l0</a><br/>\n","            "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"vkLPC43oMKbt","colab_type":"text"},"source":["### Cuda Setup"]},{"cell_type":"code","metadata":{"id":"wcq0xdkJMHUi","colab_type":"code","colab":{}},"source":["if use_cuda and not torch.cuda.is_available():\n","  print (\"WARNING: cuda is not available\")\n","  use_cuda = False"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z7Pn2e8f_TMi","colab_type":"text"},"source":["### Colab Display Setup"]},{"cell_type":"code","metadata":{"id":"KbUTeZiR_TaT","colab_type":"code","colab":{}},"source":["from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()\n","import os\n","# This code creates a virtual display to draw game images on. \n","# If you are running locally, just ignore it\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n","    !bash ../xvfb start\n","    %env DISPLAY=:1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oZXrhsrs5u-5","colab_type":"text"},"source":["## Replay Buffer"]},{"cell_type":"code","metadata":{"id":"MVMD7AwZ6gIj","colab_type":"code","colab":{}},"source":["class ReplayBuffer(object):\n","  def __init__(self, size):\n","    self.replayBuffer = deque(maxlen=size)\n","  \n","  def add(self, current_state, action, reward, next_state, done):\n","    self.replayBuffer.append((state, action, reward, next_state, done))\n","\n","  def sample(self, numSamples):\n","    state, action, reward, next_state, done = zip(*random.sample(self.replayBuffer, numSamples))\n","  \n","    return state, action, reward, next_state, done\n","  \n","  def __len__(self):\n","    return len(self.replayBuffer)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mlgDZ2Cob2ia","colab_type":"text"},"source":["###Epsilon Decay\n","- Decaying epsilon per episode\n","- Using a class so that different epsilon decay methods could be swapped in and out\n","\n"]},{"cell_type":"code","metadata":{"id":"IroaWW3rb6IW","colab_type":"code","colab":{}},"source":["class EpsilonDecay(object):\n","  def __init__(self, start_epsilon, final_epsilon, decay_epsilon):\n","    self.start_epsilon = start_epsilon\n","    self.final_epsilon = final_epsilon\n","    self.decay_epsilon = decay_epsilon\n","    self.current_epsilon = start_epsilon\n","  \n","  def decay(self):\n","    if self.current_epsilon > self.final_epsilon:\n","      self.current_epsilon *= self.decay_epsilon\n","    return self.current_epsilon\n","  \n","  def get_epsilon(self):\n","    return self.current_epsilon\n","    \n","  def reset(self, new_epsilon=None):\n","    if new_epsilon is None:\n","      self.current_epsilon = self.start_epsilon\n","    else:\n","      self.current_epsilon = new_epsilon\n","\n","    return self.current_epsilon"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5TqOmxi6G4pT","colab_type":"text"},"source":["## Deep Q Network"]},{"cell_type":"code","metadata":{"id":"dqk_Oh14HByU","colab_type":"code","outputId":"28b7e91c-011e-486d-8ad3-d545d55c31b5","executionInfo":{"status":"ok","timestamp":1588557598931,"user_tz":300,"elapsed":1572,"user":{"displayName":"Jonathan Ou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2yyYoMRykV8sYjocGFoZDy0c7Y1DdSb0H74orJQ=s64","userId":"01702736591762830230"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["class testDQN(nn.Module):\n","    def __init__(self, num_inputs, num_actions, replay_buffer, epsilon, gamma, learning_rate):\n","        super(testDQN, self).__init__()\n","        self.replay_buffer = replay_buffer # Replay Buffer\n","        self.num_actions = num_actions\n","        self.epsilon = epsilon\n","        self.gamma = gamma\n","    \n","        self.layers = nn.Sequential(\n","            nn.Linear(env.observation_space.shape[0], 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, env.action_space.n)\n","        )\n","\n","        self.optimizer = optim.Adam(self.layers.parameters(), lr=learning_rate)\n","\n","    def forward(self, x):\n","      return self.layers(x)\n","\n","    def action(self, state):\n","        if np.random.rand() <= self.epsilon.get_epsilon():  \n","            return random.randrange(self.num_actions)\n","        # Greedy Action\n","        state = torch.cuda.FloatTensor(state) if use_cuda else torch.FloatTensor(state)\n","        q_values = self.forward(state)\n","        values, indices = q_values.max(0)\n","        return indices.item()\n","\n","    def memorize(self, state, action, reward, next_state, done):\n","        self.replay_buffer.add(state, action, reward, next_state, done)\n","\n","    def learn(self, batch):\n","        if len(self.replay_buffer) < batch:\n","            return\n","        state, action, reward, next_state, done = self.replay_buffer.sample(batch)\n","        state      = torch.cuda.FloatTensor(state) if use_cuda else torch.FloatTensor(state)\n","        action     = torch.cuda.LongTensor(action) if use_cuda else torch.LongTensor(action)\n","        reward     = torch.cuda.LongTensor(reward) if use_cuda else torch.LongTensor(reward)\n","        next_state = torch.cuda.FloatTensor(next_state) if use_cuda else torch.FloatTensor(next_state) \n","        done       = torch.cuda.LongTensor(done) if use_cuda else torch.LongTensor(done)\n","\n","        # Get Q values of each state\n","        q_values      = self.forward(state)\n","        next_q_values = self.forward(next_state)\n","\n","        # Get Q Values of State-Action pair\n","        q_values = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n","        # Get Max Q Values of Next State\n","        next_q_values = next_q_values.max(1)[0]\n","        # Calculate Expected Q Value (If done, we do not have a next q value so it needs to be 0)\n","        expected_q_values = reward + self.gamma * next_q_values * (1 - done)\n","        # Fit Model\n","        loss = ((q_values - expected_q_values.data) ** 2).mean()\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","        wandb.log({\"Loss\": loss}, commit=False)\n","\n","\n"," class DQN(nn.Module):\n","    def __init__(self, num_inputs, num_actions, replay_buffer, epsilon, gamma, learning_rate):\n","        super(testDQN, self).__init__()\n","        self.replay_buffer = replay_buffer # Replay Buffer\n","        self.num_actions = num_actions\n","        self.epsilon = epsilon\n","        self.gamma = gamma\n","    \n","        self.features = nn.Sequential(\n","            nn.Conv2d(env.observation_space.shape[0], 32, kernel_size=8, stride=4),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n","            nn.ReLU()\n","        )\n","        \n","        self.fc = nn.Sequential(\n","            nn.Linear(self.feature_size(), 512),\n","            nn.ReLU(),\n","            nn.Linear(512, env.action_space.n)\n","        )\n","\n","        self.optimizer = optim.Adam(self.layers.parameters(), lr=learning_rate)\n","\n","    def feature_size(self):\n","        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n","        \n","    def forward(self, x):\n","      return self.layers(x)\n","\n","    def action(self, state):\n","        if np.random.rand() <= self.epsilon.get_epsilon():  \n","            return random.randrange(self.num_actions)\n","        # Greedy Action\n","        state = torch.cuda.FloatTensor(state) if use_cuda else torch.FloatTensor(state)\n","        q_values = self.forward(state)\n","        values, indices = q_values.max(0)\n","        return indices.item()\n","\n","    def memorize(self, state, action, reward, next_state, done):\n","        self.replay_buffer.add(state, action, reward, next_state, done)\n","\n","    def learn(self, batch):\n","        if len(self.replay_buffer) < batch:\n","            return\n","        state, action, reward, next_state, done = self.replay_buffer.sample(batch)\n","        state      = torch.cuda.FloatTensor(state) if use_cuda else torch.FloatTensor(state)\n","        action     = torch.cuda.LongTensor(action) if use_cuda else torch.LongTensor(action)\n","        reward     = torch.cuda.LongTensor(reward) if use_cuda else torch.LongTensor(reward)\n","        next_state = torch.cuda.FloatTensor(next_state) if use_cuda else torch.FloatTensor(next_state) \n","        done       = torch.cuda.LongTensor(done) if use_cuda else torch.LongTensor(done)\n","\n","        # Get Q values of each state\n","        q_values      = self.forward(state)\n","        next_q_values = self.forward(next_state)\n","\n","        # Get Q Values of State-Action pair\n","        q_values = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n","        # Get Max Q Values of Next State\n","        next_q_values = next_q_values.max(1)[0]\n","        # Calculate Expected Q Value (If done, we do not have a next q value so it needs to be 0)\n","        expected_q_values = reward + self.gamma * next_q_values * (1 - done)\n","        # Fit Model\n","        loss = ((q_values - expected_q_values.data) ** 2).mean()\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","        wandb.log({\"Loss\": loss}, commit=False)       \n","\n","\n","'''class DQN(nn.Module):\n","  def __init__(self, numInputs, numActions):\n","    self.layers == nn.Sequential(\n","        # 16 8x8 filters, stride 4 -> rectifier nonlinearity\n","        nn.Conv2d(numInputs, 32, kernel_size=(8,8), stride=4),\n","        nn.ReLu(),\n","        # 32 4x4 filter, stride 2 -> rectifier nonlinearity\n","        nn.Conv2d(32,64, kernel_size=(4,4), stride=2),\n","        nn.ReLU(),\n","        # fully connected, 256 rectifier\n","        nn.Linear(64, 256),\n","        nn.ReLU(),\n","        # fully connected  \n","        nn.Linear(256, numActions)\n","    )\n","        \n","  def forward(self, x):\n","    x = self.layers(x)\n","    return x\n","  \n","  #def act(self, state, epsilon):\n","    #if random.random() > epsilon):'''\n","    "],"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'class DQN(nn.Module):\\n  def __init__(self, numInputs, numActions):\\n    self.layers == nn.Sequential(\\n        # 16 8x8 filters, stride 4 -> rectifier nonlinearity\\n        nn.Conv2d(numInputs, 32, kernel_size=(8,8), stride=4),\\n        nn.ReLu(),\\n        # 32 4x4 filter, stride 2 -> rectifier nonlinearity\\n        nn.Conv2d(32,64, kernel_size=(4,4), stride=2),\\n        nn.ReLU(),\\n        # fully connected, 256 rectifier\\n        nn.Linear(64, 256),\\n        nn.ReLU(),\\n        # fully connected  \\n        nn.Linear(256, numActions)\\n    )\\n        \\n  def forward(self, x):\\n    x = self.layers(x)\\n    return x\\n  \\n  #def act(self, state, epsilon):\\n    #if random.random() > epsilon):'"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"65rmgRddnb61","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9304b010-a131-48e4-a71c-4f1a5c10f9dc","executionInfo":{"status":"ok","timestamp":1588561565061,"user_tz":300,"elapsed":1560,"user":{"displayName":"Jonathan Ou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2yyYoMRykV8sYjocGFoZDy0c7Y1DdSb0H74orJQ=s64","userId":"01702736591762830230"}}},"source":["features = nn.Sequential(\n","            nn.Conv2d(env.observation_space.shape[0], 32, kernel_size=8, stride=4),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n","            nn.ReLU()\n","        )\n","\n","x = torch.randn(1, env.observation_space.shape[0], 224, 224)\n","print (features(x).size())"],"execution_count":68,"outputs":[{"output_type":"stream","text":["torch.Size([1, 64, 24, 24])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mLGEH63Jp3zq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"94811c9c-d4aa-434d-cf9e-4d818fe775da","executionInfo":{"status":"ok","timestamp":1588561695744,"user_tz":300,"elapsed":1494,"user":{"displayName":"Jonathan Ou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2yyYoMRykV8sYjocGFoZDy0c7Y1DdSb0H74orJQ=s64","userId":"01702736591762830230"}}},"source":["64*24*24\n","7 * 7 * 64"],"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3136"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"markdown","metadata":{"id":"aXuOF9BU9I7C","colab_type":"text"},"source":["### Cart Pole Environment"]},{"cell_type":"code","metadata":{"id":"ssZpv1dl9LMZ","colab_type":"code","colab":{}},"source":["env = gym.make(\"CartPole-v0\")\n","env = gym.wrappers.Monitor(env, video_dir, video_callable=lambda episode_id: episode_id%500==0, force=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5XtHGdfx8LEr","colab_type":"text"},"source":["### Algorithm 1 - Deep Q-Learning with Experience Replay"]},{"cell_type":"code","metadata":{"id":"ZFSdKSMNbNeW","colab_type":"code","outputId":"c8446f1c-e566-4e6b-b495-62711774846f","executionInfo":{"status":"error","timestamp":1588559819364,"user_tz":300,"elapsed":2080058,"user":{"displayName":"Jonathan Ou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2yyYoMRykV8sYjocGFoZDy0c7Y1DdSb0H74orJQ=s64","userId":"01702736591762830230"}},"colab":{"base_uri":"https://localhost:8080/","height":375}},"source":["N = 1000 # Replay Buffer Capacity\n","M = 10000  # Number of Episodes\n","T = 1000  # Number of frames\n","LR = 0.001 # Learning Rate\n","MB = 32 # Mini batch of samples\n","\n","# Initialize replay memory D to capacity N\n","replay_buffer = ReplayBuffer(N)\n","# Initialize action-value function Q with random weights\n","epsilon = EpsilonDecay(1.0, 0.01, 0.996)\n","model = testDQN(env.observation_space.shape[0], env.action_space.n, replay_buffer, epsilon, 0.99, 0.00001)\n","if use_cuda:\n","  model = model.cuda()\n","\n","# for episode = 1, M do\n","for episode in range(M):\n","  # Initialize sequence s1 = {x} and preprocessed sequenced\n","  state = env.reset()\n","  episode_reward = 0\n","  # for t = 1, T do\n","  for t in range(T):\n","    # With probability e select a random action a otherwise select a = max(Q)\n","    action = model.action(state)\n","    # Execute action in emulator and observe reward and image\n","    next_state, reward, done, _ = env.step (action)\n","    # Store transition in D \n","    model.memorize(state, action, reward, next_state, done)\n","    # Set S(t+1) = st, at, x(t+1) and preprocess \n","    state = next_state\n","    episode_reward += reward\n","    # Sample Random minibatch of transitions from D\n","    model.learn(MB)\n","    if done:\n","      wandb.log({})\n","      break;\n","    epsilon.decay()\n","  #wanb_epsilon =  epsilon.get_epsilon()\n","  wandb.log({\"Epsilon\": epsilon.get_epsilon(), \"Reward\": episode_reward})\n","\n","env.close()"],"execution_count":58,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-58-608326fa2fd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Sample Random minibatch of transitions from D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-56-8912fe0fb193>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mexpected_q_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"g9whXQLsjrgs","colab_type":"text"},"source":["### Hyperparameters\n","\n","#### Grid Search, Random Search, Bayesian Optimization, Hyperband\n","#### [Parameter Prioritization](https://www.wandb.com/articles/running-hyperparameter-sweeps-to-pick-the-best-model-using-w-b)\n","\n","### learning rate, loss function, layer_size\n","### weight initialization, model depth, layer params, weight of regularization\n","### optimizer choice, optimize params, batch size, nonlinearity\n","\n"]},{"cell_type":"code","metadata":{"id":"mxRPg53smEai","colab_type":"code","colab":{}},"source":["## If you want to see while running - super slow\n","#\n","\n","#plt.imshow(env.render('rgb_array'))\n","#ipythondisplay.clear_output(wait=True)\n","#ipythondisplay.display(plt.gcf())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sv1DUzSG7mUD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}